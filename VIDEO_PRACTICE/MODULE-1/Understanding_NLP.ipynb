{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***`Understand NLP`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reading the files from current location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [file_name.split(\"\\\\\")[-1] for file_name in glob.glob(os.getcwd()+'\\\\doc*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc1.txt', 'doc2.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the corpus from files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PlaintextCorpusReader(root=os.getcwd(),fileids=file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc1.txt', 'doc2.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Courpus Paras`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['\"', 'My', 'Name', 'is', 'Rajesh', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'working', 'on', 'data', 'Science', 'projects', '.'], ['\",', '\"', 'The', 'nexon', 'car', 'is', 'very', 'affordable', '.'], ['\",', '\"', 'The', 'pizza', 'was', 'cheap', ',', 'tasty', 'and', 'delicious', '.'], ['\",', '\"', 'The', 'dominoz', 'pizza', 'is', 'tasty', 'and', 'loaded', '.\"']], [['\"', 'My', 'Name', 'is', 'Raman', 'Revti', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'doing', 'data', 'analytics', '.'], ['\",', '\"', 'The', 'tata', 'nexon', 'car', 'is', 'very', 'stylish', ',', 'dynamic', 'and', 'has', 'a', 'strong', 'build', '.'], ['But', 'their', 'after', 'sales', 'service', 'is', 'not', 'good', '.'], ['\",', '\"', 'The', 'pizza', 'in', 'the', 'party', 'was', 'tasty', 'and', 'cheesy', '.'], ['\",', '\"', 'The', 'dominoz', 'tacco', 'is', 'always', 'cripy', 'and', 'fingerlicious', '.\"']]]\n"
     ]
    }
   ],
   "source": [
    "print([para for para in corpus.paras()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Courpus Sentences`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\"', 'My', 'Name', 'is', 'Rajesh', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'working', 'on', 'data', 'Science', 'projects', '.'], ['\",', '\"', 'The', 'nexon', 'car', 'is', 'very', 'affordable', '.'], ['\",', '\"', 'The', 'pizza', 'was', 'cheap', ',', 'tasty', 'and', 'delicious', '.'], ['\",', '\"', 'The', 'dominoz', 'pizza', 'is', 'tasty', 'and', 'loaded', '.\"'], ['\"', 'My', 'Name', 'is', 'Raman', 'Revti', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'doing', 'data', 'analytics', '.'], ['\",', '\"', 'The', 'tata', 'nexon', 'car', 'is', 'very', 'stylish', ',', 'dynamic', 'and', 'has', 'a', 'strong', 'build', '.'], ['But', 'their', 'after', 'sales', 'service', 'is', 'not', 'good', '.'], ['\",', '\"', 'The', 'pizza', 'in', 'the', 'party', 'was', 'tasty', 'and', 'cheesy', '.'], ['\",', '\"', 'The', 'dominoz', 'tacco', 'is', 'always', 'cripy', 'and', 'fingerlicious', '.\"']]\n"
     ]
    }
   ],
   "source": [
    "corpus_sents = [sent for sent in corpus.sents()]\n",
    "print(corpus_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Courpus Words`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'My', 'Name', 'is', 'Rajesh', 'Sharma', '.\",', '\"', 'I', 'love', 'working', 'on', 'data', 'Science', 'projects', '.\",', '\"', 'The', 'nexon', 'car', 'is', 'very', 'affordable', '.\",', '\"', 'The', 'pizza', 'was', 'cheap', ',', 'tasty', 'and', 'delicious', '.\",', '\"', 'The', 'dominoz', 'pizza', 'is', 'tasty', 'and', 'loaded', '.\"', '\"', 'My', 'Name', 'is', 'Raman', 'Revti', 'Sharma', '.\",', '\"', 'I', 'love', 'doing', 'data', 'analytics', '.\",', '\"', 'The', 'tata', 'nexon', 'car', 'is', 'very', 'stylish', ',', 'dynamic', 'and', 'has', 'a', 'strong', 'build', '.', 'But', 'their', 'after', 'sales', 'service', 'is', 'not', 'good', '.\",', '\"', 'The', 'pizza', 'in', 'the', 'party', 'was', 'tasty', 'and', 'cheesy', '.\",', '\"', 'The', 'dominoz', 'tacco', 'is', 'always', 'cripy', 'and', 'fingerlicious', '.\"']\n"
     ]
    }
   ],
   "source": [
    "corpus_words = [word for word in corpus.words()]\n",
    "print(corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`English Stopwords`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['not','nor','no']:\n",
    "    eng_stopwords.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Cleaning the Corpus`***\n",
    "- ##### **Removing special characters**\n",
    "- ##### **Removing unwanted spaces**\n",
    "- ##### **Lower case the words**\n",
    "- ##### **Tokenizing the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\"', 'My', 'Name', 'is', 'Rajesh', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'working', 'on', 'data', 'Science', 'projects', '.'], ['\",', '\"', 'The', 'nexon', 'car', 'is', 'very', 'affordable', '.'], ['\",', '\"', 'The', 'pizza', 'was', 'cheap', ',', 'tasty', 'and', 'delicious', '.'], ['\",', '\"', 'The', 'dominoz', 'pizza', 'is', 'tasty', 'and', 'loaded', '.\"'], ['\"', 'My', 'Name', 'is', 'Raman', 'Revti', 'Sharma', '.'], ['\",', '\"', 'I', 'love', 'doing', 'data', 'analytics', '.'], ['\",', '\"', 'The', 'tata', 'nexon', 'car', 'is', 'very', 'stylish', ',', 'dynamic', 'and', 'has', 'a', 'strong', 'build', '.'], ['But', 'their', 'after', 'sales', 'service', 'is', 'not', 'good', '.'], ['\",', '\"', 'The', 'pizza', 'in', 'the', 'party', 'was', 'tasty', 'and', 'cheesy', '.'], ['\",', '\"', 'The', 'dominoz', 'tacco', 'is', 'always', 'cripy', 'and', 'fingerlicious', '.\"']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my name is rajesh sharma'], ['i love working on data science projects'], ['the nexon car is very affordable'], ['the pizza was cheap tasty and delicious'], ['the dominoz pizza is tasty and loaded'], ['my name is raman revti sharma'], ['i love doing data analytics'], ['the tata nexon car is very stylish dynamic and has a strong build'], ['but their after sales service is not good'], ['the pizza in the party was tasty and cheesy'], ['the dominoz tacco is always cripy and fingerlicious']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_sent = []\n",
    "for sent in corpus_sents:\n",
    "    process_sent = [re.sub('[^A-Za-z]+', ' ', str(sent)).strip().lower()]\n",
    "    cleaned_sent.append(process_sent)\n",
    "    \n",
    "print(cleaned_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Storing text messages in a DataFrame`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message\n",
       "0    0                                           my name is rajesh sharma\n",
       "1    1                            i love working on data science projects\n",
       "2    2                                   the nexon car is very affordable\n",
       "3    3                            the pizza was cheap tasty and delicious\n",
       "4    4                              the dominoz pizza is tasty and loaded\n",
       "5    5                                      my name is raman revti sharma\n",
       "6    6                                        i love doing data analytics\n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build\n",
       "8    8                          but their after sales service is not good\n",
       "9    9                        the pizza in the party was tasty and cheesy\n",
       "10  10                the dominoz tacco is always cripy and fingerlicious"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.DataFrame(cleaned_sent).reset_index()\n",
    "text_df.columns = ['Id','Pre_processed_Message']\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Removing Stopwords and Tokenization`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "      <td>[love, working, data, science, projects]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "      <td>[nexon, car, affordable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "      <td>[pizza, cheap, tasty, delicious]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "      <td>[dominoz, pizza, tasty, loaded]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "      <td>[love, data, analytics]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "      <td>[tata, nexon, car, stylish, dynamic, strong, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "      <td>[sales, service, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "      <td>[pizza, party, tasty, cheesy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "      <td>[dominoz, tacco, always, cripy, fingerlicious]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message  \\\n",
       "0    0                                           my name is rajesh sharma   \n",
       "1    1                            i love working on data science projects   \n",
       "2    2                                   the nexon car is very affordable   \n",
       "3    3                            the pizza was cheap tasty and delicious   \n",
       "4    4                              the dominoz pizza is tasty and loaded   \n",
       "5    5                                      my name is raman revti sharma   \n",
       "6    6                                        i love doing data analytics   \n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build   \n",
       "8    8                          but their after sales service is not good   \n",
       "9    9                        the pizza in the party was tasty and cheesy   \n",
       "10  10                the dominoz tacco is always cripy and fingerlicious   \n",
       "\n",
       "                                                 Tokens  \n",
       "0                                [name, rajesh, sharma]  \n",
       "1              [love, working, data, science, projects]  \n",
       "2                              [nexon, car, affordable]  \n",
       "3                      [pizza, cheap, tasty, delicious]  \n",
       "4                       [dominoz, pizza, tasty, loaded]  \n",
       "5                          [name, raman, revti, sharma]  \n",
       "6                               [love, data, analytics]  \n",
       "7   [tata, nexon, car, stylish, dynamic, strong, build]  \n",
       "8                           [sales, service, not, good]  \n",
       "9                         [pizza, party, tasty, cheesy]  \n",
       "10       [dominoz, tacco, always, cripy, fingerlicious]  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Tokens'] = text_df['Pre_processed_Message'].apply(lambda row: [word for word in row.split(\" \") if word not in eng_stopwords])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Stemming`***\n",
    "#### ***`Porter Stemmer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Porter_Stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "      <td>[love, working, data, science, projects]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "      <td>[nexon, car, affordable]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "      <td>[pizza, cheap, tasty, delicious]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "      <td>[dominoz, pizza, tasty, loaded]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "      <td>[love, data, analytics]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "      <td>[tata, nexon, car, stylish, dynamic, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "      <td>[sales, service, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "      <td>[pizza, party, tasty, cheesy]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "      <td>[dominoz, tacco, always, cripy, fingerlicious]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message  \\\n",
       "0    0                                           my name is rajesh sharma   \n",
       "1    1                            i love working on data science projects   \n",
       "2    2                                   the nexon car is very affordable   \n",
       "3    3                            the pizza was cheap tasty and delicious   \n",
       "4    4                              the dominoz pizza is tasty and loaded   \n",
       "5    5                                      my name is raman revti sharma   \n",
       "6    6                                        i love doing data analytics   \n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build   \n",
       "8    8                          but their after sales service is not good   \n",
       "9    9                        the pizza in the party was tasty and cheesy   \n",
       "10  10                the dominoz tacco is always cripy and fingerlicious   \n",
       "\n",
       "                                                 Tokens  \\\n",
       "0                                [name, rajesh, sharma]   \n",
       "1              [love, working, data, science, projects]   \n",
       "2                              [nexon, car, affordable]   \n",
       "3                      [pizza, cheap, tasty, delicious]   \n",
       "4                       [dominoz, pizza, tasty, loaded]   \n",
       "5                          [name, raman, revti, sharma]   \n",
       "6                               [love, data, analytics]   \n",
       "7   [tata, nexon, car, stylish, dynamic, strong, build]   \n",
       "8                           [sales, service, not, good]   \n",
       "9                         [pizza, party, tasty, cheesy]   \n",
       "10       [dominoz, tacco, always, cripy, fingerlicious]   \n",
       "\n",
       "                                         Porter_Stems  \n",
       "0                              [name, rajesh, sharma]  \n",
       "1                 [love, work, data, scienc, project]  \n",
       "2                                [nexon, car, afford]  \n",
       "3                       [pizza, cheap, tasti, delici]  \n",
       "4                       [dominoz, pizza, tasti, load]  \n",
       "5                        [name, raman, revti, sharma]  \n",
       "6                                [love, data, analyt]  \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]  \n",
       "8                           [sale, servic, not, good]  \n",
       "9                       [pizza, parti, tasti, cheesi]  \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Porter_Stems'] = text_df['Tokens'].apply(lambda row: [port_stem.stem(word) for word in row])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Porter Stemmer is simply chopping off the tails of the words. Not a good way!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Snowball Stemmer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stem = SnowballStemmer(language='english',ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Porter_Stems</th>\n",
       "      <th>Snowball_Stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "      <td>[love, working, data, science, projects]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "      <td>[nexon, car, affordable]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "      <td>[pizza, cheap, tasty, delicious]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "      <td>[dominoz, pizza, tasty, loaded]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "      <td>[love, data, analytics]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "      <td>[tata, nexon, car, stylish, dynamic, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "      <td>[sales, service, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "      <td>[pizza, party, tasty, cheesy]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "      <td>[dominoz, tacco, always, cripy, fingerlicious]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message  \\\n",
       "0    0                                           my name is rajesh sharma   \n",
       "1    1                            i love working on data science projects   \n",
       "2    2                                   the nexon car is very affordable   \n",
       "3    3                            the pizza was cheap tasty and delicious   \n",
       "4    4                              the dominoz pizza is tasty and loaded   \n",
       "5    5                                      my name is raman revti sharma   \n",
       "6    6                                        i love doing data analytics   \n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build   \n",
       "8    8                          but their after sales service is not good   \n",
       "9    9                        the pizza in the party was tasty and cheesy   \n",
       "10  10                the dominoz tacco is always cripy and fingerlicious   \n",
       "\n",
       "                                                 Tokens  \\\n",
       "0                                [name, rajesh, sharma]   \n",
       "1              [love, working, data, science, projects]   \n",
       "2                              [nexon, car, affordable]   \n",
       "3                      [pizza, cheap, tasty, delicious]   \n",
       "4                       [dominoz, pizza, tasty, loaded]   \n",
       "5                          [name, raman, revti, sharma]   \n",
       "6                               [love, data, analytics]   \n",
       "7   [tata, nexon, car, stylish, dynamic, strong, build]   \n",
       "8                           [sales, service, not, good]   \n",
       "9                         [pizza, party, tasty, cheesy]   \n",
       "10       [dominoz, tacco, always, cripy, fingerlicious]   \n",
       "\n",
       "                                         Porter_Stems  \\\n",
       "0                              [name, rajesh, sharma]   \n",
       "1                 [love, work, data, scienc, project]   \n",
       "2                                [nexon, car, afford]   \n",
       "3                       [pizza, cheap, tasti, delici]   \n",
       "4                       [dominoz, pizza, tasti, load]   \n",
       "5                        [name, raman, revti, sharma]   \n",
       "6                                [love, data, analyt]   \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]   \n",
       "8                           [sale, servic, not, good]   \n",
       "9                       [pizza, parti, tasti, cheesi]   \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]   \n",
       "\n",
       "                                       Snowball_Stems  \n",
       "0                              [name, rajesh, sharma]  \n",
       "1                 [love, work, data, scienc, project]  \n",
       "2                                [nexon, car, afford]  \n",
       "3                       [pizza, cheap, tasti, delici]  \n",
       "4                       [dominoz, pizza, tasti, load]  \n",
       "5                        [name, raman, revti, sharma]  \n",
       "6                                [love, data, analyt]  \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]  \n",
       "8                           [sale, servic, not, good]  \n",
       "9                       [pizza, parti, tasti, cheesi]  \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Snowball_Stems'] = text_df['Tokens'].apply(lambda row: [snow_stem.stem(word) for word in row])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Snowball Stemmer is considered as a strongest method for taking a word to its root form. However, here, I didn't oberserved any difference with Porter Stemmer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Lancaster Stemmer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanc_stem = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Porter_Stems</th>\n",
       "      <th>Snowball_Stems</th>\n",
       "      <th>Lancaster_Stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[nam, rajesh, sharm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "      <td>[love, working, data, science, projects]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "      <td>[lov, work, dat, sci, project]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "      <td>[nexon, car, affordable]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "      <td>[pizza, cheap, tasty, delicious]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "      <td>[pizz, cheap, tasty, delicy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "      <td>[dominoz, pizza, tasty, loaded]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "      <td>[dominoz, pizz, tasty, load]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[nam, ram, revt, sharm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "      <td>[love, data, analytics]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "      <td>[lov, dat, analys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "      <td>[tata, nexon, car, stylish, dynamic, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "      <td>[tat, nexon, car, styl, dynam, strong, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "      <td>[sales, service, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "      <td>[sal, serv, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "      <td>[pizza, party, tasty, cheesy]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "      <td>[pizz, party, tasty, cheesy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "      <td>[dominoz, tacco, always, cripy, fingerlicious]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "      <td>[dominoz, tacco, alway, cripy, fingerlicy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message  \\\n",
       "0    0                                           my name is rajesh sharma   \n",
       "1    1                            i love working on data science projects   \n",
       "2    2                                   the nexon car is very affordable   \n",
       "3    3                            the pizza was cheap tasty and delicious   \n",
       "4    4                              the dominoz pizza is tasty and loaded   \n",
       "5    5                                      my name is raman revti sharma   \n",
       "6    6                                        i love doing data analytics   \n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build   \n",
       "8    8                          but their after sales service is not good   \n",
       "9    9                        the pizza in the party was tasty and cheesy   \n",
       "10  10                the dominoz tacco is always cripy and fingerlicious   \n",
       "\n",
       "                                                 Tokens  \\\n",
       "0                                [name, rajesh, sharma]   \n",
       "1              [love, working, data, science, projects]   \n",
       "2                              [nexon, car, affordable]   \n",
       "3                      [pizza, cheap, tasty, delicious]   \n",
       "4                       [dominoz, pizza, tasty, loaded]   \n",
       "5                          [name, raman, revti, sharma]   \n",
       "6                               [love, data, analytics]   \n",
       "7   [tata, nexon, car, stylish, dynamic, strong, build]   \n",
       "8                           [sales, service, not, good]   \n",
       "9                         [pizza, party, tasty, cheesy]   \n",
       "10       [dominoz, tacco, always, cripy, fingerlicious]   \n",
       "\n",
       "                                         Porter_Stems  \\\n",
       "0                              [name, rajesh, sharma]   \n",
       "1                 [love, work, data, scienc, project]   \n",
       "2                                [nexon, car, afford]   \n",
       "3                       [pizza, cheap, tasti, delici]   \n",
       "4                       [dominoz, pizza, tasti, load]   \n",
       "5                        [name, raman, revti, sharma]   \n",
       "6                                [love, data, analyt]   \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]   \n",
       "8                           [sale, servic, not, good]   \n",
       "9                       [pizza, parti, tasti, cheesi]   \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]   \n",
       "\n",
       "                                       Snowball_Stems  \\\n",
       "0                              [name, rajesh, sharma]   \n",
       "1                 [love, work, data, scienc, project]   \n",
       "2                                [nexon, car, afford]   \n",
       "3                       [pizza, cheap, tasti, delici]   \n",
       "4                       [dominoz, pizza, tasti, load]   \n",
       "5                        [name, raman, revti, sharma]   \n",
       "6                                [love, data, analyt]   \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]   \n",
       "8                           [sale, servic, not, good]   \n",
       "9                       [pizza, parti, tasti, cheesi]   \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]   \n",
       "\n",
       "                                  Lancaster_Stems  \n",
       "0                            [nam, rajesh, sharm]  \n",
       "1                  [lov, work, dat, sci, project]  \n",
       "2                            [nexon, car, afford]  \n",
       "3                    [pizz, cheap, tasty, delicy]  \n",
       "4                    [dominoz, pizz, tasty, load]  \n",
       "5                         [nam, ram, revt, sharm]  \n",
       "6                              [lov, dat, analys]  \n",
       "7   [tat, nexon, car, styl, dynam, strong, build]  \n",
       "8                          [sal, serv, not, good]  \n",
       "9                    [pizz, party, tasty, cheesy]  \n",
       "10     [dominoz, tacco, alway, cripy, fingerlicy]  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Lancaster_Stems'] = text_df['Tokens'].apply(lambda row: [lanc_stem.stem(word) for word in row])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definitely Lancaster Stemmer is not a good approach to bring a word to its root form. It is chopping-off the vowels from the tails of the words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Lemmatizers`***\n",
    "##### **Reference Links**\n",
    "\n",
    "- https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    \n",
    "- https://www.nltk.org/book/ch05.html\n",
    "\n",
    "\n",
    "#### ***`Wordnet Lemmatizer`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = (\"Waaooo, what a beautifull fight. I really enjoy watching WWE and pro-wrestling. And, they are my stress busters too.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnet_lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('really', 'RB'),\n",
       " ('enjoy', 'VB'),\n",
       " ('watching', 'VBG'),\n",
       " ('WWE', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('pro-wrestling.', 'JJ'),\n",
       " ('And,', 'NNP'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('my', 'PRP$'),\n",
       " ('stress', 'JJ'),\n",
       " ('busters', 'NNS'),\n",
       " ('too.', 'VBP')]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(test_sentence.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(test_sentence.split(\" \"))[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I really enjoy watching WWE and pro-wrestling. And, they are my stress busters too.'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag)#, wordnet.ADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordnet_pos('beautifull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautifull'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnet_lemma.lemmatize('beautifull',get_wordnet_pos('beautifull'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python -m pip install treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "word='beauti'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = TextBlob(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Waaooo what a beautifull fight I really enjoy watching WWE and pro-wrestling And they are my stress buster too'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \". join([w.lemmatize() for w in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treetaggerwrapper as ttpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = ttpw.TreeTagger(TAGLANG='en',TAGDIR=\"C:\\TreeTagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = tagger.tag_text(text=test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [t.split('\\t')[-1] for t in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Waaooo',\n",
       " ',',\n",
       " 'what',\n",
       " 'a',\n",
       " 'beautifull',\n",
       " 'fight',\n",
       " '.',\n",
       " 'I',\n",
       " 'really',\n",
       " 'enjoy',\n",
       " 'watch',\n",
       " 'WWE',\n",
       " 'and',\n",
       " 'pro-wrestling',\n",
       " '.',\n",
       " 'and',\n",
       " ',',\n",
       " 'they',\n",
       " 'be',\n",
       " 'my',\n",
       " 'stress',\n",
       " 'buster',\n",
       " 'too',\n",
       " '.']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pre_processed_Message</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Porter_Stems</th>\n",
       "      <th>Snowball_Stems</th>\n",
       "      <th>Lancaster_Stems</th>\n",
       "      <th>Wordnet_Lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>my name is rajesh sharma</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[name, rajesh, sharma]</td>\n",
       "      <td>[nam, rajesh, sharm]</td>\n",
       "      <td>[nam, rajesh, sharm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i love working on data science projects</td>\n",
       "      <td>[love, working, data, science, projects]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "      <td>[love, work, data, scienc, project]</td>\n",
       "      <td>[lov, work, dat, sci, project]</td>\n",
       "      <td>[lov, work, dat, sci, project]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the nexon car is very affordable</td>\n",
       "      <td>[nexon, car, affordable]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "      <td>[nexon, car, afford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the pizza was cheap tasty and delicious</td>\n",
       "      <td>[pizza, cheap, tasty, delicious]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "      <td>[pizza, cheap, tasti, delici]</td>\n",
       "      <td>[pizz, cheap, tasty, delicy]</td>\n",
       "      <td>[pizz, cheap, tasty, delicy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the dominoz pizza is tasty and loaded</td>\n",
       "      <td>[dominoz, pizza, tasty, loaded]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "      <td>[dominoz, pizza, tasti, load]</td>\n",
       "      <td>[dominoz, pizz, tasty, load]</td>\n",
       "      <td>[dominoz, pizz, tasty, load]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>my name is raman revti sharma</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[name, raman, revti, sharma]</td>\n",
       "      <td>[nam, ram, revt, sharm]</td>\n",
       "      <td>[nam, ram, revt, sharm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>i love doing data analytics</td>\n",
       "      <td>[love, data, analytics]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "      <td>[love, data, analyt]</td>\n",
       "      <td>[lov, dat, analys]</td>\n",
       "      <td>[lov, dat, analys]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>the tata nexon car is very stylish dynamic and has a strong build</td>\n",
       "      <td>[tata, nexon, car, stylish, dynamic, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "      <td>[tata, nexon, car, stylish, dynam, strong, build]</td>\n",
       "      <td>[tat, nexon, car, styl, dynam, strong, build]</td>\n",
       "      <td>[tat, nexon, car, styl, dynam, strong, build]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>but their after sales service is not good</td>\n",
       "      <td>[sales, service, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "      <td>[sale, servic, not, good]</td>\n",
       "      <td>[sal, serv, not, good]</td>\n",
       "      <td>[sal, serv, not, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>the pizza in the party was tasty and cheesy</td>\n",
       "      <td>[pizza, party, tasty, cheesy]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "      <td>[pizza, parti, tasti, cheesi]</td>\n",
       "      <td>[pizz, party, tasty, cheesy]</td>\n",
       "      <td>[pizz, party, tasty, cheesy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>the dominoz tacco is always cripy and fingerlicious</td>\n",
       "      <td>[dominoz, tacco, always, cripy, fingerlicious]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "      <td>[dominoz, tacco, alway, cripi, fingerlici]</td>\n",
       "      <td>[dominoz, tacco, alway, cripy, fingerlicy]</td>\n",
       "      <td>[dominoz, tacco, alway, cripy, fingerlicy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                              Pre_processed_Message  \\\n",
       "0    0                                           my name is rajesh sharma   \n",
       "1    1                            i love working on data science projects   \n",
       "2    2                                   the nexon car is very affordable   \n",
       "3    3                            the pizza was cheap tasty and delicious   \n",
       "4    4                              the dominoz pizza is tasty and loaded   \n",
       "5    5                                      my name is raman revti sharma   \n",
       "6    6                                        i love doing data analytics   \n",
       "7    7  the tata nexon car is very stylish dynamic and has a strong build   \n",
       "8    8                          but their after sales service is not good   \n",
       "9    9                        the pizza in the party was tasty and cheesy   \n",
       "10  10                the dominoz tacco is always cripy and fingerlicious   \n",
       "\n",
       "                                                 Tokens  \\\n",
       "0                                [name, rajesh, sharma]   \n",
       "1              [love, working, data, science, projects]   \n",
       "2                              [nexon, car, affordable]   \n",
       "3                      [pizza, cheap, tasty, delicious]   \n",
       "4                       [dominoz, pizza, tasty, loaded]   \n",
       "5                          [name, raman, revti, sharma]   \n",
       "6                               [love, data, analytics]   \n",
       "7   [tata, nexon, car, stylish, dynamic, strong, build]   \n",
       "8                           [sales, service, not, good]   \n",
       "9                         [pizza, party, tasty, cheesy]   \n",
       "10       [dominoz, tacco, always, cripy, fingerlicious]   \n",
       "\n",
       "                                         Porter_Stems  \\\n",
       "0                              [name, rajesh, sharma]   \n",
       "1                 [love, work, data, scienc, project]   \n",
       "2                                [nexon, car, afford]   \n",
       "3                       [pizza, cheap, tasti, delici]   \n",
       "4                       [dominoz, pizza, tasti, load]   \n",
       "5                        [name, raman, revti, sharma]   \n",
       "6                                [love, data, analyt]   \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]   \n",
       "8                           [sale, servic, not, good]   \n",
       "9                       [pizza, parti, tasti, cheesi]   \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]   \n",
       "\n",
       "                                       Snowball_Stems  \\\n",
       "0                              [name, rajesh, sharma]   \n",
       "1                 [love, work, data, scienc, project]   \n",
       "2                                [nexon, car, afford]   \n",
       "3                       [pizza, cheap, tasti, delici]   \n",
       "4                       [dominoz, pizza, tasti, load]   \n",
       "5                        [name, raman, revti, sharma]   \n",
       "6                                [love, data, analyt]   \n",
       "7   [tata, nexon, car, stylish, dynam, strong, build]   \n",
       "8                           [sale, servic, not, good]   \n",
       "9                       [pizza, parti, tasti, cheesi]   \n",
       "10         [dominoz, tacco, alway, cripi, fingerlici]   \n",
       "\n",
       "                                  Lancaster_Stems  \\\n",
       "0                            [nam, rajesh, sharm]   \n",
       "1                  [lov, work, dat, sci, project]   \n",
       "2                            [nexon, car, afford]   \n",
       "3                    [pizz, cheap, tasty, delicy]   \n",
       "4                    [dominoz, pizz, tasty, load]   \n",
       "5                         [nam, ram, revt, sharm]   \n",
       "6                              [lov, dat, analys]   \n",
       "7   [tat, nexon, car, styl, dynam, strong, build]   \n",
       "8                          [sal, serv, not, good]   \n",
       "9                    [pizz, party, tasty, cheesy]   \n",
       "10     [dominoz, tacco, alway, cripy, fingerlicy]   \n",
       "\n",
       "                                   Wordnet_Lemmas  \n",
       "0                            [nam, rajesh, sharm]  \n",
       "1                  [lov, work, dat, sci, project]  \n",
       "2                            [nexon, car, afford]  \n",
       "3                    [pizz, cheap, tasty, delicy]  \n",
       "4                    [dominoz, pizz, tasty, load]  \n",
       "5                         [nam, ram, revt, sharm]  \n",
       "6                              [lov, dat, analys]  \n",
       "7   [tat, nexon, car, styl, dynam, strong, build]  \n",
       "8                          [sal, serv, not, good]  \n",
       "9                    [pizz, party, tasty, cheesy]  \n",
       "10     [dominoz, tacco, alway, cripy, fingerlicy]  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Wordnet_Lemmas'] = text_df[''].apply(lambda row: [wnet_lemma.lemmatize(word) for word in row])\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***`Featurization`***\n",
    "### **1. BAG of WORDS (BOW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[['name', 'rajesh', 'sharma'], ['love', 'working', 'data', 'science', 'projects'], ['nexon', 'car', 'affordable'], ['pizza', 'cheap', 'tasty', 'delicious'], ['dominoz', 'pizza', 'tasty', 'loaded'], ['name', 'raman', 'revti', 'sharma'], ['love', 'data', 'analytics'], ['tata', 'nexon', 'car', 'stylish', 'dynamic', 'strong', 'build'], ['sales', 'service', 'not', 'good'], ['pizza', 'party', 'tasty', 'cheesy'], ['dominoz', 'tacco', 'always', 'cripy', 'fingerlicious']]\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(str(preprocess_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-35b604fdbdb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBOW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "BOW = cv.fit_transform(''.join(preprocess_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(cv.get_feature_names()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.get_stop_words())       ## Here, in countvectoriser we can also initialize the stopwords but in this case I have kept it blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_features = pd.DataFrame(BOW.toarray(),columns=cv.get_feature_names())\n",
    "bow_features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. N-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([' '.join(final_corpus_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = cv2.fit_transform([' '.join(final_corpus_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_features = pd.DataFrame(ngrams.toarray(),columns=cv2.get_feature_names())\n",
    "ngrams_features.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
